import { ts } from "../entities/at";
import { getModeName } from "../entities/prompt";
import { User } from "../entities/user";
import { gptChatCompletion } from "../external/gptChatCompletion";
import { capitalize, commatize, first, toText, truncate } from "../lib/common";
import { isSuccess } from "../lib/error";
import { clearAndLeave, encodeText, inlineKeyboard, reply, replyWithKeyboard } from "../lib/telegram";
import { storeMessage } from "../storage/messageStorage";
import { addMessageToUser, getLastHistoryMessage, getOrAddUser, stopWaitingForGptAnswer, updateUserProduct, waitForGptAnswer } from "./userService";
import { commands } from "../lib/constants";
import { formatSubscription, getCurrentSubscription } from "./subscriptionService";
import { getCurrentHistory } from "./contextService";
import { getCaseForNumber } from "./grammarService";
import { Completion } from "../entities/message";
import { putMetric } from "./metricService";
import { isDebugMode } from "./userSettingsService";
import { gptTimeout } from "./gptService";
import { happened, timeLeft } from "./dateService";
import { AnyContext } from "../telegram/botContext";
import { PureTextModelCode, TextModelCode } from "../entities/model";
import { incUsage, isUsageLimitExceeded } from "./usageStatsService";
import { getProductTypeDisplayName } from "./productService";
import { getTextModelByCode } from "./modelService";
import { incProductUsage } from "./productUsageService";
import { intervalPhrases, intervals } from "../entities/interval";
import { PurchasedProduct } from "../entities/product";
import { getIntervalString } from "./intervalService";
import { formatLimit } from "./usageLimitService";
import { remindButton } from "../lib/dialog";
import { getConsumptionLimits, isConsumptionLimit } from "./consumptionService";
import { ConsumptionLimit, IntervalConsumptionLimits } from "../entities/consumption";
import { getTextModelContexts } from "./modelContextService";

const config = {
  messageInterval: parseInt(process.env.MESSAGE_INTERVAL ?? "15") * 1000, // milliseconds
};

export async function sendMessageToGpt(ctx: AnyContext, user: User, question: string, requestedAt?: number) {
  const context = first(getTextModelContexts(user));

  if (!context) {
    throw new Error("Text model context is undefined.");
  }

  const {
    product,
    modelCode,
    pureModelCode,
    model,
    lastUsedAt,
    usagePoints
  } = context;

  if (user.waitingForGptAnswer) {
    if (lastUsedAt && happened(lastUsedAt.timestamp, gptTimeout * 1000)) {
      // we have waited enough for the GPT answer
      await stopWaitingForGptAnswer(user);
    } else {
      await reply(ctx, "–Ø –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≤–∞—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");
      return;
    }
  }

  // we check the user's usage stats if we don't use a product,
  // but fall back to the defaults
  if (!product) {
    for (const interval of intervals) {
      const phrases = intervalPhrases[interval];

      if (isUsageLimitExceeded(user, pureModelCode, interval)) {
        await reply(
          ctx,
          `–í—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ ${phrases.current}. ${phrases.smilies}`,
          `–ü–æ–¥–æ–∂–¥–∏—Ç–µ ${phrases.next} –∏–ª–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Ç–∞—Ä–∏—Ñ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º –ª–∏–º–∏—Ç–æ–º: /${commands.premium}`
        );
  
        return;
      }
    }
  }

  if (config.messageInterval > 0 && lastUsedAt) {
    const seconds = Math.ceil(
      timeLeft(lastUsedAt.timestamp, config.messageInterval) / 1000
    );

    if (seconds > 0) {
      await reply(
        ctx,
        `–í—ã –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ ${seconds} ${getCaseForNumber("—Å–µ–∫—É–Ω–¥–∞", seconds)}... ‚è≥`
      );

      return;
    }
  }

  const messages = await reply(ctx, "ü§î –î—É–º–∞—é –Ω–∞–¥ –æ—Ç–≤–µ—Ç–æ–º, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");

  await waitForGptAnswer(user);
  const answer = await gptChatCompletion(user, model, question);
  await stopWaitingForGptAnswer(user);

  await ctx.deleteMessage(messages[0].message_id);

  const now = ts();

  const message = await storeMessage(
    user,
    question,
    answer,
    requestedAt ?? now,
    now
  );

  if (isSuccess(answer)) {
    await addMessageToUser(user, message);

    await reply(
      ctx,
      answer.reply
        ? formatGptMessage(answer.reply)
        : "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç ChatGPT. üò£"
    );

    if (product) {
      product.usage = incProductUsage(
        product.usage,
        modelCode,
        usagePoints
      );

      user = await updateUserProduct(user, product);
    }

    user = await incUsage(user, pureModelCode, message.requestedAt);
  } else {
    let errorMessage = answer.message;

    if (errorMessage.includes("Please reduce the length of the messages.")) {
      errorMessage = "–í—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å.";
    } else if (errorMessage.includes("Rate limit reached")) {
      errorMessage = "–í—ã —à–ª–µ—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥... ‚è≥";
    } else if (errorMessage.includes("model is currently overloaded")) {
      errorMessage = "–û–π, —á—Ç–æ-—Ç–æ –º–Ω–µ –ø–æ–ø–ª–æ—Ö–µ–ª–æ... üòµ –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞. –î–∞–π—Ç–µ –æ—Ç–¥—ã—à–∞—Ç—å—Å—è... ‚è≥";
    } else if (errorMessage.includes("The server had an error while processing your request")) {
      errorMessage = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ ChatGPT. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å.";
    }

    await reply(ctx, `‚ùå ${errorMessage}`);
  }

  const formattedLimits = formatConsumptionLimits(user, product, modelCode, pureModelCode);

  const info = buildInfo(
    user,
    modelCode,
    isSuccess(answer) ? answer : null,
    formattedLimits
  );

  await reply(ctx, info);

  if (isSuccess(answer)) {
    await addMessageMetrics(answer);
  }
}

export async function showStatus(ctx: AnyContext, user: User) {
  await replyWithKeyboard(
    ctx,
    inlineKeyboard(remindButton),
    getStatusMessage(user)
  );
}

export function getStatusMessage(user: User): string {
  const subscription = getCurrentSubscription(user);

  return toText(
    `${capitalize(getProductTypeDisplayName(subscription))}: ${formatSubscription(subscription)}`,
    `–†–µ–∂–∏–º: <b>${getModeName(user)}</b>`
  );
}

export async function showLastHistoryMessage(ctx: AnyContext, user: User, fallbackMessage?: string) {
  const historyMessage = getLastHistoryMessage(user);

  const message = historyMessage
    ? formatGptMessage(historyMessage)
    : fallbackMessage;

  if (message) {
    await reply(ctx, message);
  }
}

export async function getUserOrLeave(ctx: AnyContext): Promise<User | null> {
  if (ctx.from) {
    return await getOrAddUser(ctx.from);
  }

  if (ctx.scene) {
    await clearAndLeave(ctx);
  }

  console.error(new Error("User not found (empty ctx.from)."));
  await putMetric("Error");
  await putMetric("UserNotFoundError");

  return null;
}

function formatConsumptionLimits(
  user: User,
  product: PurchasedProduct | null,
  modelCode: TextModelCode,
  pureModelCode: PureTextModelCode
): string | null {
  const limits = getConsumptionLimits(user, product, modelCode, pureModelCode);

  if (!limits) {
    return null;
  }

  const what = modelCode === "gptokens" ? "–≥–ø—Ç–æ–∫–µ–Ω–æ–≤" : "–∑–∞–ø—Ä–æ—Å–æ–≤";

  return isConsumptionLimit(limits)
    ? formatConsumptionLimit(limits, what)
    : formatIntervalConsumptionLimits(limits, what);
}

function formatConsumptionLimit({ limit, remaining }: ConsumptionLimit, what: string): string {
  return `–æ—Å—Ç–∞–ª–æ—Å—å ${what}: ${remaining}/${formatLimit(limit)}`;
}

function formatIntervalConsumptionLimits(limits: IntervalConsumptionLimits, what: string): string {
  const chunks: string[] = [];

  for (const { interval, limit, remaining } of limits) {
    chunks.push(
      `–æ—Å—Ç–∞–ª–æ—Å—å ${what} –≤ ${getIntervalString(interval, "Accusative")}: ${remaining}/${formatLimit(limit)}`
    );
  }

  return commatize(chunks);
}

async function addMessageMetrics(completion: Completion) {
  await putMetric("MessageSent");

  if (completion.usage) {
    await putMetric("TokensUsed", completion.usage.totalTokens);
  }
}

function formatGptMessage(message: string): string {
    return `ü§ñ ${encodeText(message)}`;
}

function buildInfo(
  user: User,
  modelCode: TextModelCode,
  answer: Completion | null,
  formattedLimits: string | null
) {
  const model = getTextModelByCode(modelCode);
  const chunks: string[] = [];

  chunks.push(`üìå –†–µ–∂–∏–º: <b>${getModeName(user)}</b>`);

  if (isDebugMode(user)) {
    if (answer?.usage) {
      const usage = answer.usage;
      chunks.push(`—Ç–æ–∫–µ–Ω—ã: ${usage.totalTokens} (${usage.promptTokens} + ${usage.completionTokens})`);
    }

    const context = user.context;

    if (context) {
      const messages = getCurrentHistory(context).messages;

      if (messages.length) {
        const truncatedRequests = messages.map(m => `[${truncate(m.request, 20)}]`);

        chunks.push(
          encodeText(
            `–∏—Å—Ç–æ—Ä–∏—è: ${commatize(truncatedRequests)}`
          )
        );
      } else {
        chunks.push("–∏—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞");
      }
    }

    chunks.push(`–º–æ–¥–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞: ${model}`);

    if (answer?.model) {
      chunks.push(`–º–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∞: ${answer.model}`);
    }
  }

  if (formattedLimits) {
    chunks.push(formattedLimits);
  }

  return commatize(chunks);
}
