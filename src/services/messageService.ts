import { ts } from "../entities/at";
import { getModeName } from "../entities/prompt";
import { User } from "../entities/user";
import { gptChatCompletion } from "../external/gptChatCompletion";
import { commatize, truncate } from "../lib/common";
import { isSuccess } from "../lib/error";
import { clearAndLeave, encodeText, reply } from "../lib/telegram";
import { storeMessage } from "../storage/messageStorage";
import { addMessageToUser, getLastHistoryMessage, getOrAddUser, getUserActiveProduct, stopWaitingForGptAnswer, updateUserProduct, waitForGptAnswer } from "./userService";
import { commands } from "../lib/constants";
import { formatSubscription, getCurrentSubscription } from "./subscriptionService";
import { getCurrentHistory } from "./contextService";
import { getCaseByNumber } from "./grammarService";
import { Completion } from "../entities/message";
import { putMetric } from "./metricService";
import { isDebugMode } from "./userSettingsService";
import { gptTimeout } from "./gptService";
import { happened, timeLeft } from "./dateService";
import { AnyContext } from "../telegram/botContext";
import { TextModelCode, defaultTextModelCode } from "../entities/model";
import { getLastUsedAt, incUsage, isUsageLimitExceeded } from "./usageStatsService";
import { getAvailableTextModel, getProductTypeDisplayName } from "./productService";
import { getTextModelByCode, purifyTextModelCode } from "./modelService";
import { incProductUsage } from "./productUsageService";
import { getTextModelUsagePoints } from "./modelUsageService";
import { getUsageReport } from "./usageService";

const config = {
  messageInterval: parseInt(process.env.MESSAGE_INTERVAL ?? "15") * 1000, // milliseconds
};

export async function sendMessageToGpt(ctx: AnyContext, user: User, question: string, requestedAt?: number) {
  const activeProduct = getUserActiveProduct(user);

  const productModelCode = activeProduct
    ? getAvailableTextModel(activeProduct)
    : null;

  const usingProduct = !!activeProduct && !!productModelCode;

  const modelCode = productModelCode ?? defaultTextModelCode;
  const pureModelCode = purifyTextModelCode(modelCode);
  const model = getTextModelByCode(modelCode);

  const lastUsedAt = getLastUsedAt(user.usageStats, pureModelCode);

  if (user.waitingForGptAnswer) {
    if (lastUsedAt && happened(lastUsedAt.timestamp, gptTimeout * 1000)) {
      // we have waited enough for the GPT answer
      await stopWaitingForGptAnswer(user);
    } else {
      await reply(ctx, "–Ø –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≤–∞—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");
      return;
    }
  }

  // we check the user's usage stats if we don't use a product,
  // but fall back to the defaults
  if (!usingProduct) {
    if (isUsageLimitExceeded(user, pureModelCode, "day")) {
      await reply(
        ctx,
        "–í—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ —Å–µ–≥–æ–¥–Ω—è. üò•",
        `–ü–æ–¥–æ–∂–¥–∏—Ç–µ –¥–æ –∑–∞–≤—Ç—Ä–∞ –∏–ª–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Ç–∞—Ä–∏—Ñ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º –ª–∏–º–∏—Ç–æ–º: /${commands.premium}`
      );

      return;
    }

    if (isUsageLimitExceeded(user, pureModelCode, "week")) {
      await reply(
        ctx,
        "–í—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ —ç—Ç—É –Ω–∞–¥–µ–ª—é. üò•üò•",
        `–ü–æ–¥–æ–∂–¥–∏—Ç–µ —Å–ª–µ–¥—É—é—â–µ–π –Ω–µ–¥–µ–ª–∏ –∏–ª–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Ç–∞—Ä–∏—Ñ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º –ª–∏–º–∏—Ç–æ–º: /${commands.premium}`
      );

      return;
    }

    if (isUsageLimitExceeded(user, pureModelCode, "month")) {
      await reply(
        ctx,
        "–í—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ —ç—Ç–æ—Ç –º–µ—Å—è—Ü. üò•üò•üò•",
        `–ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ —Å–ª–µ–¥—É—é—â–µ–π –º–µ—Å—è—Ü–µ –∏–ª–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Ç–∞—Ä–∏—Ñ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º –ª–∏–º–∏—Ç–æ–º: /${commands.premium}`
      );

      return;
    }
  }

  if (config.messageInterval > 0 && lastUsedAt) {
    const seconds = Math.ceil(
      timeLeft(lastUsedAt.timestamp, config.messageInterval) / 1000
    );

    if (seconds > 0) {
      await reply(
        ctx,
        `–í—ã –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ ${seconds} ${getCaseByNumber("—Å–µ–∫—É–Ω–¥–∞", seconds)}... ‚è≥`
      );

      return;
    }
  }

  const messages = await reply(ctx, "ü§î –î—É–º–∞—é –Ω–∞–¥ –æ—Ç–≤–µ—Ç–æ–º, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");

  await waitForGptAnswer(user);
  const answer = await gptChatCompletion(user, model, question);
  await stopWaitingForGptAnswer(user);

  await ctx.deleteMessage(messages[0].message_id);

  const now = ts();

  const message = await storeMessage(
    user,
    question,
    answer,
    requestedAt ?? now,
    now
  );

  if (isSuccess(answer)) {
    await addMessageToUser(user, message);

    await reply(
      ctx,
      answer.reply
        ? formatGptMessage(answer.reply)
        : "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç ChatGPT. üò£"
    );

    if (usingProduct) {
      const usagePoints = getTextModelUsagePoints(modelCode);

      activeProduct.usage = incProductUsage(
        activeProduct.usage,
        modelCode,
        usagePoints
      );

      user = await updateUserProduct(user, activeProduct);
    }

    user = await incUsage(user, pureModelCode, message.requestedAt);
  } else {
    let errorMessage = answer.message;

    if (errorMessage.includes("Please reduce the length of the messages.")) {
      errorMessage = "–í—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å.";
    } else if (errorMessage.includes("Rate limit reached")) {
      errorMessage = "–í—ã —à–ª–µ—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥... ‚è≥";
    } else if (errorMessage.includes("model is currently overloaded")) {
      errorMessage = "–û–π, —á—Ç–æ-—Ç–æ –º–Ω–µ –ø–æ–ø–ª–æ—Ö–µ–ª–æ... üòµ –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞. –î–∞–π—Ç–µ –æ—Ç–¥—ã—à–∞—Ç—å—Å—è... ‚è≥";
    } else if (errorMessage.includes("The server had an error while processing your request")) {
      errorMessage = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ ChatGPT. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å.";
    }

    await reply(ctx, `‚ùå ${errorMessage}`);
  }

  const usageReport = getUsageReport(
    user,
    modelCode,
    pureModelCode,
    usingProduct ? activeProduct : null
  );

  const info = buildInfo(
    user,
    modelCode,
    isSuccess(answer) ? answer : null,
    usageReport
  );

  await reply(ctx, info);

  if (isSuccess(answer)) {
    await addMessageMetrics(answer);
  }
}

export async function showStatus(ctx: AnyContext, user: User) {
  const subscription = getCurrentSubscription(user);

  await reply(ctx, `–¢–µ–∫—É—â–∏–π ${getProductTypeDisplayName(subscription)}: ${formatSubscription(subscription)}`);
  await reply(ctx, `–¢–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º: <b>${getModeName(user)}</b>`);
  await showLastHistoryMessage(ctx, user);
}

export async function showLastHistoryMessage(ctx: AnyContext, user: User, fallbackMessage?: string) {
  const historyMessage = getLastHistoryMessage(user);

  const message = historyMessage
    ? formatGptMessage(historyMessage)
    : fallbackMessage;

  if (message) {
    await reply(ctx, message);
  }
}

export async function getUserOrLeave(ctx: AnyContext): Promise<User | null> {
  if (ctx.from) {
    return await getOrAddUser(ctx.from);
  }

  if (ctx.scene) {
    await clearAndLeave(ctx);
  }

  console.error(new Error("User not found (empty ctx.from)."));
  await putMetric("Error");
  await putMetric("UserNotFoundError");

  return null;
}

async function addMessageMetrics(completion: Completion) {
  await putMetric("MessageSent");

  if (completion.usage) {
    await putMetric("TokensUsed", completion.usage.totalTokens);
  }
}

function formatGptMessage(message: string): string {
    return `ü§ñ ${encodeText(message)}`;
}

function buildInfo(
  user: User,
  modelCode: TextModelCode,
  answer: Completion | null,
  usageReport: string | null
) {
  const model = getTextModelByCode(modelCode);
  const chunks: string[] = [];

  chunks.push(`üìå –†–µ–∂–∏–º: <b>${getModeName(user)}</b>`);

  if (isDebugMode(user)) {
    if (answer?.usage) {
      const usage = answer.usage;
      chunks.push(`—Ç–æ–∫–µ–Ω—ã: ${usage.totalTokens} (${usage.promptTokens} + ${usage.completionTokens})`);
    }

    const context = user.context;

    if (context) {
      const messages = getCurrentHistory(context).messages;

      if (messages.length) {
        const truncatedRequests = messages.map(m => `[${truncate(m.request, 20)}]`);

        chunks.push(
          encodeText(
            `–∏—Å—Ç–æ—Ä–∏—è: ${commatize(truncatedRequests)}`
          )
        );
      } else {
        chunks.push("–∏—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞");
      }
    }

    chunks.push(`–º–æ–¥–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞: ${model}`);

    if (answer?.model) {
      chunks.push(`–º–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∞: ${answer.model}`);
    }
  }

  if (usageReport) {
    chunks.push(usageReport);
  }

  return commatize(chunks);
}
