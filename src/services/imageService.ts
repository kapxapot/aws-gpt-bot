import { now } from "../entities/at";
import { ImageSettings, defaultImageSize } from "../entities/model";
import { User } from "../entities/user";
import { gptImageGeneration } from "../external/gptImageGeneration";
import { formatWordNumber } from "./grammarService";
import { backToStartAction, cancelButton } from "../lib/dialog";
import { isSuccess } from "../lib/error";
import { inlineKeyboard, reply, replyWithKeyboard } from "../lib/telegram";
import { storeImageRequest, updateImageRequest } from "../storage/imageRequestStorage";
import { BotContext } from "../telegram/botContext";
import { isIntervalElapsed, timeLeft } from "./dateService";
import { gptTimeout } from "./gptService";
import { putMetric } from "./metricService";
import { incUsage } from "./usageStatsService";
import { stopWaitingForGptImageGeneration, updateUserProduct, waitForGptImageGeneration } from "./userService";
import { incProductUsage } from "./productUsageService";
import { Markup } from "telegraf";
import { ImageModelContext } from "../entities/modelContext";
import { symbols } from "../lib/constants";
import { toText } from "../lib/text";

const config = {
  imageInterval: parseInt(process.env.IMAGE_INTERVAL ?? "60") * 1000, // milliseconds
};

export async function generateImageWithGpt(
  ctx: BotContext,
  imageModelContext: ImageModelContext,
  user: User,
  prompt: string
): Promise<boolean> {
  const requestedAt = now();

  const {
    product,
    modelCode,
    pureModelCode,
    model,
    lastUsedAt,
    imageSettings,
    usagePoints
  } = imageModelContext;

  if (user.waitingForGptImageGeneration) {
    if (lastUsedAt && isIntervalElapsed(lastUsedAt.timestamp, gptTimeout * 1000)) {
      // we have waited enough for the GPT answer
      await stopWaitingForGptImageGeneration(user);
    } else {
      await reply(ctx, "–í–∞—à–∞ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");
      return false;
    }
  }

  if (config.imageInterval > 0 && lastUsedAt) {
    const seconds = Math.ceil(
      timeLeft(lastUsedAt.timestamp, config.imageInterval) / 1000
    );

    if (seconds > 0) {
      await reply(
        ctx,
        `–í—ã –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç–µ –∑–∞–ø—Ä–æ—Å—ã —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ ${formatWordNumber("—Å–µ–∫—É–Ω–¥–∞", seconds)}... ‚è≥`
      );

      return false;
    }
  }

  const messages = await reply(ctx, "üë®‚Äçüé® –†–∏—Å—É—é –≤–∞—à—É –∫–∞—Ä—Ç–∏–Ω–∫—É, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");

  let imageRequest = await storeImageRequest({
    userId: user.id,
    model,
    size: imageSettings.size,
    quality: imageSettings.quality,
    style: imageSettings.style,
    prompt,
    responseFormat: "url",
    requestedAt,
    strict: true
  });

  await waitForGptImageGeneration(user);
  const image = await gptImageGeneration(imageRequest);
  await stopWaitingForGptImageGeneration(user);

  await ctx.deleteMessage(messages[0].message_id);

  imageRequest = await updateImageRequest(
    imageRequest,
    {
      response: image,
      respondedAt: now()
    }
  );

  if (isSuccess(image)) {
    const url = image.url;

    console.log(url); // to debug the broken OpenAI urls

    if (!url) {
      console.error("The generated image is missing `url`.");
      await putMetric("Error");
      await putMetric("ImageHasNoUrlError");

      await reply(
        ctx,
        "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Ä–∏—Å–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
      );

      return false;
    }

    await reply(
      ctx,
      `${symbols.picture} –í–∞—à–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É <b>¬´${prompt}¬ª</b> –≥–æ—Ç–æ–≤–∞. üëá`
    );

    await ctx.replyWithPhoto(url);

    await ctx.replyWithHTML(
      toText(
        `<a href="${url}">–°–∫–∞—á–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É</a> –≤ —Ö–æ—Ä–æ—à–µ–º –∫–∞—á–µ—Å—Ç–≤–µ.`,
        `${symbols.warning} –°—Å—ã–ª–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç 60 –º–∏–Ω—É—Ç!`
      ),
      {
        ...inlineKeyboard(
          Markup.button.url("–°–∫–∞—á–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É", url),
          [`–°–æ–∑–¥–∞—Ç—å –µ—â–µ –æ–¥–Ω—É ${symbols.picture}`, backToStartAction],
          cancelButton
        ),
        disable_web_page_preview: true
      }
    );

    if (product) {
      product.usage = incProductUsage(
        product.usage,
        modelCode,
        usagePoints
      );

      user = await updateUserProduct(user, product);
    }

    user = await incUsage(user, pureModelCode, requestedAt);

    await putMetric("ImageGenerated");

    return true;
  } else {
    let errorMessage = image.message;

    if (errorMessage.includes("Your prompt may contain text that is not allowed by our safety system.")) {
      errorMessage = "–í–∞—à –∑–∞–ø—Ä–æ—Å –±—ã–ª –æ—Ç–≤–µ—Ä–≥–Ω—É—Ç —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å.";
    } else if (errorMessage.includes("This request has been blocked by our content filters.")) {
      errorMessage = "–í–∞—à –∑–∞–ø—Ä–æ—Å –±—ã–ª –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å.";
    } else if (errorMessage.includes("Image descriptions generated from your prompt may contain text that is not allowed by our safety system. If you believe this was done in error, your request may succeed if retried, or by adjusting your prompt.")) {
      errorMessage = "–í–∞—à –∑–∞–ø—Ä–æ—Å –±—ã–ª –æ—Ç–≤–µ—Ä–≥–Ω—É—Ç —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ OpenAI. –ï—Å–ª–∏ –≤—ã —Å—á–∏—Ç–∞–µ—Ç–µ, —á—Ç–æ —ç—Ç–æ –æ—à–∏–±–∫–∞, –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å. –ò–Ω–∞—á–µ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å.";
    }

    await replyWithKeyboard(
      ctx,
      inlineKeyboard(cancelButton),
      `${symbols.cross} ${errorMessage}`
    );
  }

  return false;
}

export const getDefaultImageSettings = (): ImageSettings => ({
  size: defaultImageSize
});

export const imageSettingsEqual = (settingsA: ImageSettings, settingsB: ImageSettings) =>
  settingsA.size === settingsB.size && settingsA.quality === settingsB.quality;
