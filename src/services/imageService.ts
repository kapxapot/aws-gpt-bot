import { now } from "../entities/at";
import { ImageModelCode, ImageQuality, ImageSize, defaultImageModelCode, defaultImageSize } from "../entities/model";
import { User } from "../entities/user";
import { gptImageGeneration } from "../external/gptImageGeneration";
import { getCaseByNumber } from "./grammarService";
import { commatize, toText } from "../lib/common";
import { commands } from "../lib/constants";
import { cancelButton } from "../lib/dialog";
import { isSuccess } from "../lib/error";
import { inlineKeyboard, reply, replyWithKeyboard } from "../lib/telegram";
import { storeImageRequest, updateImageRequest } from "../storage/imageRequestStorage";
import { AnyContext } from "../telegram/botContext";
import { happened, timeLeft } from "./dateService";
import { gptTimeout } from "./gptService";
import { putMetric } from "./metricService";
import { getLastUsedAt, incUsage, isUsageLimitExceeded } from "./usageStatsService";
import { getUserActiveProduct, stopWaitingForGptImageGeneration, updateUserProduct, waitForGptImageGeneration } from "./userService";
import { PassThrough } from "stream";
import { getAvailableImageModel } from "./productService";
import { getImageModelByCode, purifyImageModelCode } from "./modelService";
import { incProductUsage } from "./productUsageService";
import { getImageModelUsagePoints } from "./modelUsageService";
import { getUsageReport } from "./usageService";
import { getModeName } from "../entities/prompt";
import { isDebugMode } from "./userSettingsService";

const config = {
  imageInterval: parseInt(process.env.IMAGE_INTERVAL ?? "60") * 1000, // milliseconds
};

export async function generateImageWithGpt(ctx: AnyContext, user: User, prompt: string): Promise<boolean> {
  const requestedAt = now();
  const activeProduct = getUserActiveProduct(user);

  const productModelCode = activeProduct
    ? getAvailableImageModel(activeProduct)
    : null;

  const usingProduct = activeProduct && productModelCode;

  const modelCode = productModelCode ?? defaultImageModelCode;
  const pureModelCode = purifyImageModelCode(modelCode);
  const model = getImageModelByCode(modelCode);

  const lastUsedAt = getLastUsedAt(user.usageStats, pureModelCode);

  if (user.waitingForGptImageGeneration) {
    if (lastUsedAt && happened(lastUsedAt.timestamp, gptTimeout * 1000)) {
      // we have waited enough for the GPT answer
      await stopWaitingForGptImageGeneration(user);
    } else {
      await reply(ctx, "–í–∞—à–∞ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");
      return false;
    }
  }

  // we check the user's usage stats if we don't use a product,
  // but fall back to the defaults
  if (!usingProduct) {
    if (isUsageLimitExceeded(user, pureModelCode, "day")) {
      await reply(
        ctx,
        "–í—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è. üò•",
        `–ü–æ–¥–æ–∂–¥–∏—Ç–µ –¥–æ –∑–∞–≤—Ç—Ä–∞ –∏–ª–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Ç–∞—Ä–∏—Ñ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º –ª–∏–º–∏—Ç–æ–º: /${commands.premium}`
      );
  
      return false;
    }

    if (isUsageLimitExceeded(user, pureModelCode, "week")) {
      await reply(
        ctx,
        "–í—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–∞ —ç—Ç—É –Ω–µ–¥–µ–ª—é. üò•üò•",
        `–ü–æ–¥–æ–∂–¥–∏—Ç–µ —Å–ª–µ–¥—É—é—â–µ–π –Ω–µ–¥–µ–ª–∏ –∏–ª–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Ç–∞—Ä–∏—Ñ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º –ª–∏–º–∏—Ç–æ–º: /${commands.premium}`
      );
  
      return false;
    }

    if (isUsageLimitExceeded(user, pureModelCode, "month")) {
      await reply(
        ctx,
        "–í—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–∞ —ç—Ç–æ—Ç –º–µ—Å—è—Ü. üò•üò•üò•",
        `–ü–æ–¥–æ–∂–¥–∏—Ç–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –º–µ—Å—è—Ü–∞ –∏–ª–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Ç–∞—Ä–∏—Ñ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º –ª–∏–º–∏—Ç–æ–º: /${commands.premium}`
      );
  
      return false;
    }
  }

  if (config.imageInterval > 0 && lastUsedAt) {
    const seconds = Math.ceil(
      timeLeft(lastUsedAt.timestamp, config.imageInterval) / 1000
    );

    if (seconds > 0) {
      await reply(
        ctx,
        `–í—ã –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç–µ –∑–∞–ø—Ä–æ—Å—ã —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ ${seconds} ${getCaseByNumber("—Å–µ–∫—É–Ω–¥–∞", seconds)}... ‚è≥`
      );

      return false;
    }
  }

  const messages = await reply(ctx, "üë®‚Äçüé® –†–∏—Å—É—é –≤–∞—à—É –∫–∞—Ä—Ç–∏–Ω–∫—É, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");

  const imageSize: ImageSize = defaultImageSize;
  const imageQuality: ImageQuality | undefined = undefined;

  let imageRequest = await storeImageRequest({
    userId: user.id,
    model,
    size: imageSize,
    quality: imageQuality,
    prompt,
    responseFormat: "url",
    requestedAt,
    strict: false
  });

  await waitForGptImageGeneration(user);
  const image = await gptImageGeneration(imageRequest);
  await stopWaitingForGptImageGeneration(user);

  await ctx.deleteMessage(messages[0].message_id);

  // this doesn't work for "b64_json" format
  imageRequest = await updateImageRequest(
    imageRequest,
    {
      response: image,
      respondedAt: now()
    }
  );

  if (isSuccess(image)) {
    await reply(
      ctx,
      `üñº –í–∞—à–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É <b>¬´${prompt}¬ª</b> –≥–æ—Ç–æ–≤–∞. üëá`
    );

    if (image.url) {
      await ctx.replyWithPhoto(image.url);

      await ctx.replyWithHTML(
        toText(
          `–í—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ <a href="${image.url}">—Å–∫–∞—á–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É</a> –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º –∫–∞—á–µ—Å—Ç–≤–µ.`,
          "‚ö† –í–Ω–∏–º–∞–Ω–∏–µ! –≠—Ç–∞ —Å—Å—ã–ª–∫–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ 60 –º–∏–Ω—É—Ç!"
        ),
        { 
          disable_web_page_preview: true 
        }
      );
    } else if (image.b64_json) {
      const stream = new PassThrough();
      stream.write(image.b64_json);
      stream.end();

      await ctx.replyWithPhoto({ source: stream });
    }

    if (usingProduct) {
      const usagePoints = getImageModelUsagePoints(modelCode, imageSize, imageQuality);

      activeProduct.usage = incProductUsage(
        activeProduct.usage,
        modelCode,
        usagePoints
      );

      user = await updateUserProduct(user, activeProduct);
    }

    user = await incUsage(user, pureModelCode, requestedAt);

    const usageReport = getUsageReport(
      user,
      modelCode,
      pureModelCode,
      usingProduct ? activeProduct : null
    );
  
    const info = buildInfo(
      user,
      modelCode,
      usageReport
    );
  
    await reply(ctx, info);
  
    await putMetric("ImageGenerated");

    return true;
  } else {
    let errorMessage = image.message;

    if (errorMessage.includes("Your prompt may contain text that is not allowed by our safety system.")) {
      errorMessage = "–í–∞—à –∑–∞–ø—Ä–æ—Å –±—ã–ª –æ—Ç–≤–µ—Ä–≥–Ω—É—Ç —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å.";
    } else if (errorMessage.includes("This request has been blocked by our content filters.")) {
      errorMessage = "–í–∞—à –∑–∞–ø—Ä–æ—Å –±—ã–ª –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å.";
    } else if (errorMessage.includes("Image descriptions generated from your prompt may contain text that is not allowed by our safety system. If you believe this was done in error, your request may succeed if retried, or by adjusting your prompt.")) {
      errorMessage = "–í–∞—à –∑–∞–ø—Ä–æ—Å –±—ã–ª –æ—Ç–≤–µ—Ä–≥–Ω—É—Ç —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ OpenAI. –ï—Å–ª–∏ –≤—ã —Å—á–∏—Ç–∞–µ—Ç–µ, —á—Ç–æ —ç—Ç–æ –æ—à–∏–±–∫–∞, –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å. –ò–Ω–∞—á–µ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å.";
    }

    await replyWithKeyboard(
      ctx,
      inlineKeyboard(cancelButton),
      `‚ùå ${errorMessage}`
    );
  }

  return false;
}

function buildInfo(
  user: User,
  modelCode: ImageModelCode,
  usageReport: string | null
) {
  const model = getImageModelByCode(modelCode);
  const chunks: string[] = [];

  chunks.push(`üìå –†–µ–∂–∏–º: <b>${getModeName(user)}</b>`);

  if (isDebugMode(user)) {
    chunks.push(`–º–æ–¥–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞: ${model}`);
  }

  if (usageReport) {
    chunks.push(usageReport);
  }

  return commatize(chunks);
}
