import { now } from "../entities/at";
import { ImageSettings, defaultImageSize } from "../entities/model";
import { User } from "../entities/user";
import { gptImageGeneration } from "../external/gptImageGeneration";
import { getCaseForNumber } from "./grammarService";
import { anotherImageButton, cancelButton } from "../lib/dialog";
import { isSuccess } from "../lib/error";
import { inlineKeyboard, reply, replyWithKeyboard } from "../lib/telegram";
import { storeImageRequest, updateImageRequest } from "../storage/imageRequestStorage";
import { BotContext } from "../telegram/botContext";
import { happened, timeLeft } from "./dateService";
import { gptTimeout } from "./gptService";
import { putMetric } from "./metricService";
import { incUsage } from "./usageStatsService";
import { stopWaitingForGptImageGeneration, updateUserProduct, waitForGptImageGeneration } from "./userService";
import { PassThrough } from "stream";
import { incProductUsage } from "./productUsageService";
import { toText } from "../lib/common";
import { Markup } from "telegraf";
import { ImageModelContext } from "../entities/modelContext";

const config = {
  imageInterval: parseInt(process.env.IMAGE_INTERVAL ?? "60") * 1000, // milliseconds
};

export async function generateImageWithGpt(
  ctx: BotContext,
  imageModelContext: ImageModelContext,
  user: User,
  prompt: string
): Promise<boolean> {
  const requestedAt = now();

  const {
    product,
    modelCode,
    pureModelCode,
    model,
    lastUsedAt,
    imageSettings,
    usagePoints
  } = imageModelContext;

  if (user.waitingForGptImageGeneration) {
    if (lastUsedAt && happened(lastUsedAt.timestamp, gptTimeout * 1000)) {
      // we have waited enough for the GPT answer
      await stopWaitingForGptImageGeneration(user);
    } else {
      await reply(ctx, "–í–∞—à–∞ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");
      return false;
    }
  }

  if (config.imageInterval > 0 && lastUsedAt) {
    const seconds = Math.ceil(
      timeLeft(lastUsedAt.timestamp, config.imageInterval) / 1000
    );

    if (seconds > 0) {
      await reply(
        ctx,
        `–í—ã –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç–µ –∑–∞–ø—Ä–æ—Å—ã —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ ${seconds} ${getCaseForNumber("—Å–µ–∫—É–Ω–¥–∞", seconds)}... ‚è≥`
      );

      return false;
    }
  }

  const messages = await reply(ctx, "üë®‚Äçüé® –†–∏—Å—É—é –≤–∞—à—É –∫–∞—Ä—Ç–∏–Ω–∫—É, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥");

  let imageRequest = await storeImageRequest({
    userId: user.id,
    model,
    size: imageSettings.size,
    quality: imageSettings.quality,
    prompt,
    responseFormat: "url",
    requestedAt,
    strict: true
  });

  await waitForGptImageGeneration(user);
  const image = await gptImageGeneration(imageRequest);
  await stopWaitingForGptImageGeneration(user);

  await ctx.deleteMessage(messages[0].message_id);

  // this doesn't work for "b64_json" format
  imageRequest = await updateImageRequest(
    imageRequest,
    {
      response: image,
      respondedAt: now()
    }
  );

  if (isSuccess(image)) {
    await reply(
      ctx,
      `üñº –í–∞—à–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É <b>¬´${prompt}¬ª</b> –≥–æ—Ç–æ–≤–∞. üëá`
    );

    if (image.url) {
      await ctx.replyWithPhoto(image.url);

      await ctx.replyWithHTML(
        toText(
          `<a href="${image.url}">–°–∫–∞—á–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É</a> –≤ —Ö–æ—Ä–æ—à–µ–º –∫–∞—á–µ—Å—Ç–≤–µ.`,
          "‚ö† –°—Å—ã–ª–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç 60 –º–∏–Ω—É—Ç!"
        ),
        {
          ...inlineKeyboard(
            Markup.button.url("–°–∫–∞—á–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É", image.url),
            anotherImageButton,
            cancelButton
          ),
          disable_web_page_preview: true
        }
      );
    } else if (image.b64_json) {
      const stream = new PassThrough();
      stream.write(image.b64_json);
      stream.end();

      await ctx.replyWithPhoto({ source: stream });
    }

    if (product) {
      product.usage = incProductUsage(
        product.usage,
        modelCode,
        usagePoints
      );

      user = await updateUserProduct(user, product);
    }

    user = await incUsage(user, pureModelCode, requestedAt);

    await putMetric("ImageGenerated");

    return true;
  } else {
    let errorMessage = image.message;

    if (errorMessage.includes("Your prompt may contain text that is not allowed by our safety system.")) {
      errorMessage = "–í–∞—à –∑–∞–ø—Ä–æ—Å –±—ã–ª –æ—Ç–≤–µ—Ä–≥–Ω—É—Ç —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å.";
    } else if (errorMessage.includes("This request has been blocked by our content filters.")) {
      errorMessage = "–í–∞—à –∑–∞–ø—Ä–æ—Å –±—ã–ª –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å.";
    } else if (errorMessage.includes("Image descriptions generated from your prompt may contain text that is not allowed by our safety system. If you believe this was done in error, your request may succeed if retried, or by adjusting your prompt.")) {
      errorMessage = "–í–∞—à –∑–∞–ø—Ä–æ—Å –±—ã–ª –æ—Ç–≤–µ—Ä–≥–Ω—É—Ç —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ OpenAI. –ï—Å–ª–∏ –≤—ã —Å—á–∏—Ç–∞–µ—Ç–µ, —á—Ç–æ —ç—Ç–æ –æ—à–∏–±–∫–∞, –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å. –ò–Ω–∞—á–µ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å.";
    }

    await replyWithKeyboard(
      ctx,
      inlineKeyboard(cancelButton),
      `‚ùå ${errorMessage}`
    );
  }

  return false;
}

export const getDefaultImageSettings = (): ImageSettings => ({
  size: defaultImageSize
});

export const imageSettingsEqual = (settingsA: ImageSettings, settingsB: ImageSettings) =>
  settingsA.size === settingsB.size && settingsA.quality === settingsB.quality;
